# NbS_Scan_Project_Stocktake
**AI-assisted evidence synthesis: screening → extraction → harmonization → benchmarking**  
*January 2026*

This repository contains the scripts and outputs used for the **Rural NbS Scan Project — Phase 1 Stocktake and Benchmarking**. The goal is a **diagnostic stocktake** of methods used for **NbS spatial prioritisation / suitability / targeting** (peer-reviewed + grey literature), with emphasis on **practical methodological insight** rather than exhaustive coverage.

---

## Scope: Nature-based Solutions (NbS)

The stocktake covers **10 priority NbS practices**, grouped into functional clusters:

**Forest & Tree Systems**
- Forest conservation and restoration  
- Reforestation & afforestation  
- Community-based forest management  
- Agroforestry  
- Agrosilvopastoralism / silvopastoralism  

**Water & Wetland Systems**
- Wetland management  
- Constructed wetlands  
- Riparian buffers  
- Water harvesting and conservation  

**Managed Landscapes (cross-cutting)**
- Integrated fire management  

---

## Repository contents

### `scripts/` (R Markdown)
These scripts implement the end-to-end workflow:

- `AI_SCREEN_NBS.Rmd`  
  **AI-assisted screening** of titles/abstracts (peer-reviewed). Produces include/exclude decisions used for downstream extraction.

- `NbS_Extraction.Rmd`  
  **Information extraction (peer-reviewed)** into a structured template (methods, inputs, outputs, validation, tools, resolution, geography, climate/economics flags, etc.). Includes AI-assisted extraction steps if enabled.

- `NbS_harmonization.Rmd`  
  **Harmonization / standardization** of extracted fields into analysis-ready typologies (e.g., method families, variable groups, output categories, validation types, resolution classes).

- `Nbs_benchmarking.Rmd`  
  **Benchmarking (peer-reviewed)**: generates scalability score, evidence quality score, and composite benchmark flag (Low/Medium/High).

- `grey_lit_extraction.Rmd`  
  **Information extraction (grey literature)** into the same (or comparable) structured framework for synthesis and comparison.

- `nbs_grey_lit_benchmarking.Rmd`  
  **Benchmarking (grey literature)** using the project’s evidence-quality rules for non-peer-reviewed sources.

---

### `data/` (key outputs / intermediate data)
These are the main data products generated by the scripts:

#### Peer-reviewed
- `included.bib`  
  Bibliography file of included peer-reviewed sources (post-screening).

- `nbs_peer_reviewed_extraction_.csv`  
  Extracted fields for included peer-reviewed studies (pre-harmonization).

- `nbs_harmonized_peer_reviewed.csv`  
  Harmonized / standardized peer-reviewed extraction results (analysis-ready categories).

- `nbs_peer_reviewed_benchmarked.csv`  
  Peer-reviewed results with benchmarking scores + composite benchmark flag.

#### Grey literature
- `nbs_harmonized_grey_lit.csv`  
  Harmonized / standardized grey literature extraction results.

- `nbs_grey_lit_benchmarked.csv`  
  Grey literature results with benchmarking scores + composite benchmark flag.

> Note: file naming follows the repo as uploaded (including the trailing underscore in `nbs_peer_reviewed_extraction_.csv`).

---

## Workflow overview (how the pieces fit)

### 1) Peer-reviewed literature workflow
1. **AI screening** (`AI_SCREEN_NBS.Rmd`)  
   Screen titles/abstracts to include only papers that:
   - address one (or more) priority NbS practices, **and**
   - present a **spatial prioritisation / suitability / targeting** approach  
   Output feeds into the included set.

2. **Extraction** (`NbS_Extraction.Rmd`)  
   Extract structured fields from included sources, focusing on *methods and spatial logic*.

3. **Harmonization** (`NbS_harmonization.Rmd`)  
   Map extracted verbatim terms into shared typologies (methods/variables/outputs/validation/etc.).

4. **Benchmarking** (`Nbs_benchmarking.Rmd`)  
   Score each source along:
   - **Scalability (1–3)** (scope, transferability, data generalizability)
   - **Evidence quality (1–3)** (bibliometrics proxy)
   Then assign a composite **Benchmark Flag** (Low / Medium / High).

---

### 2) Grey literature workflow
1. **Extraction** (`grey_lit_extraction.Rmd`)  
   Extract comparable methodological details from grey literature sources.

2. **Benchmarking** (`nbs_grey_lit_benchmarking.Rmd`)  
   Apply evidence-quality rules appropriate for grey literature (e.g., major org technical guidance vs non-reviewed reports), plus scalability scoring.

---

## How to run (recommended order)

From the repo root:

1. `scripts/AI_SCREEN_NBS.Rmd`
2. `scripts/NbS_Extraction.Rmd`
3. `scripts/NbS_harmonization.Rmd`
4. `scripts/Nbs_benchmarking.Rmd`
5. `scripts/grey_lit_extraction.Rmd`
6. `scripts/nbs_grey_lit_benchmarking.Rmd`

Outputs are written to `data/` as `.csv` and `.bib`.

---

## Requirements

- R (>= 4.2 recommended)
- R packages depend on your implementation, but typically include:
  - `tidyverse`, `data.table`, `readr`, `stringr`, `jsonlite`
  - PDF/text utilities if used (e.g., `pdftools`)
  - AI screening package if used (e.g., `AIscreenR`)

<div style="display: flex; gap: 8px;">
  <img src="https://github.com/user-attachments/assets/7914560c-4856-4b0f-bb23-6d44d698ec4e" width="48%" />
  <img src="https://github.com/user-attachments/assets/91c9aaf3-e2e8-44e9-a28c-2243a815e440" width="48%" />
</div>




