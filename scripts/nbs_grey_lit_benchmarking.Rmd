---
title: "NbS Stocktake - Grey Literature Benchmarking: Scalability & Quality Scoring"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# ============================================================================
# NbS STOCKTAKE - GREY LITERATURE BENCHMARKING
# ============================================================================
#
# PURPOSE:
#   Score each grey literature document on two dimensions:
#
#   DIMENSION 1 — SCALABILITY (GPT-scored from GPT Summary text)
#     A. Geographic Scope (1–3)
#     B. Method Transferability (1–3)
#     C. Data Input Generalizability (1–3)
#     Scalability_Score = median(A, B, C)
#
#   DIMENSION 2 — EVIDENCE QUALITY (adapted for grey literature)
#     Based on: Organizational Authority + Document Rigor + Citation Count
#     Quality_Score (1–3)
#
#   COMPOSITE BENCHMARK FLAG (matrix combination)
#
# ADAPTATION FROM PEER-REVIEWED BENCHMARKING:
#   - Uses GPT Summary instead of full PDF text
#   - Replaces bibliometric (Journal IF) with organizational authority
#   - Incorporates document type and existing confidence ratings
#
# ============================================================================

```{r libraries}
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(glue)
library(readr)
library(writexl)
library(ellmer)
library(DT)
```

# ============================================================================
# PART 1: CONFIGURATION
# ============================================================================

```{r config}
# ----- API Configuration -----
openai_api_key <- Sys.getenv("NBS_API_KEY")
GPT_MODEL <- "gpt-4o-mini"  # or "gpt-4o" for higher quality

# ----- Paths -----
# UPDATE THIS PATH to your grey literature data file
GREY_LIT_FILE <- "path/to/your/grey_literature_data.csv"  # CSV or Excel

# Output directory
OUT_DIR <- "grey_lit_benchmarking_results"
dir.create(OUT_DIR, showWarnings = FALSE, recursive = TRUE)

# ----- Text truncation for API calls -----
MAX_CHARS <- 60000  # Grey lit summaries are shorter than full PDFs

# ----- Helper Functions -----
`%||%` <- function(a, b) if (!is.null(a) && !is.na(a) && nzchar(a)) a else b

clip_text <- function(x, max_chars = MAX_CHARS) {
  sapply(x, function(txt) {
    if (is.na(txt) || !nzchar(txt) || nchar(txt) <= max_chars) return(txt)
    head_chars <- round(max_chars * 0.7)
    tail_chars <- round(max_chars * 0.3)
    head_part <- substr(txt, 1, head_chars)
    tail_part <- substr(txt, nchar(txt) - tail_chars + 1, nchar(txt))
    paste0(head_part, "\n\n[... MIDDLE TRUNCATED ...]\n\n", tail_part)
  }, USE.NAMES = FALSE)
}

safe_chat <- function(prompt, model = GPT_MODEL) {
  tryCatch({
    chat <- ellmer::chat_openai(model = model)
    resp <- chat$chat(prompt)
    list(response = trimws(resp), error = NA_character_)
  }, error = function(e) {
    list(response = NA_character_, error = conditionMessage(e))
  })
}

# Parse GPT response: "justification text; SCORE"
parse_score_response <- function(response_text) {
  if (is.na(response_text) || !nzchar(response_text)) {
    return(list(score = NA_integer_, justification = NA_character_))
  }
  
  # Split on last semicolon to get justification and score
  last_semicolon <- regexpr(";[^;]*$", response_text)
  
  if (last_semicolon > 0) {
    justification <- trimws(substr(response_text, 1, last_semicolon - 1))
    score_part <- trimws(substr(response_text, last_semicolon + 1, nchar(response_text)))
  } else {
    justification <- response_text
    score_part <- response_text
  }
  
  # Extract the digit 1, 2, or 3 from score part
  score_match <- str_extract(score_part, "[123]")
  score <- if (!is.na(score_match)) as.integer(score_match) else NA_integer_
  
  if (is.na(justification) || nchar(justification) == 0) {
    justification <- NA_character_
  }
  
  list(score = score, justification = justification)
}
```

# ============================================================================
# PART 2: LOAD GREY LITERATURE DATA
# ============================================================================

```{r load_data}
# Load grey literature data
# Assumes CSV with columns matching your provided structure:
# - NbS Group, ID, Source (title), Organisation, Year, Document type
# - Geography, Spatial approach, Key spatial logic, Climate handling
# - Cost handling, Why it matters, Link, GPT Summary, citations, Confidence

if (file.exists(GREY_LIT_FILE)) {
  if (grepl("\\.xlsx$", GREY_LIT_FILE, ignore.case = TRUE)) {
    grey_lit_df <- readxl::read_excel(GREY_LIT_FILE)
  } else {
    grey_lit_df <- read_csv(GREY_LIT_FILE, show_col_types = FALSE)
  }
  message(glue("Loaded {nrow(grey_lit_df)} grey literature documents"))
} else {
  # Create example data structure for testing
  message("Grey literature file not found. Creating example structure...")
  grey_lit_df <- tibble(
    `NbS Group` = character(),
    ID = integer(),
    `Source (title)` = character(),
    `Organisation / authoring body` = character(),
    Year = integer(),
    `Document type` = character(),
    Geography = character(),
    `Spatial approach` = character(),
    `Key spatial logic (10–25 words)` = character(),
    `Climate handling (non-trivial only)` = character(),
    `Cost handling (non-trivial only)` = character(),
    `Why it matters (1 sentence)` = character(),
    Link = character(),
    `GPT Summary` = character(),
    citations = numeric(),
    Confidence = character()
  )
}

# Standardize column names for easier processing
grey_lit_df <- grey_lit_df %>%
  rename_with(~ str_replace_all(.x, "\\s+", "_")) %>%
  rename_with(~ str_replace_all(.x, "[^A-Za-z0-9_]", ""))

# Show column names for verification
cat("Columns in dataset:\n")
print(names(grey_lit_df))

# Filter to documents with GPT Summary text
docs_to_score <- grey_lit_df %>%
  filter(!is.na(GPT_Summary), nchar(GPT_Summary) > 100)

message(glue("\nDocuments ready for benchmarking: {nrow(docs_to_score)}"))
```

# ============================================================================
# PART 3: SCALABILITY PROMPTS (adapted for grey literature summaries)
# ============================================================================

```{r prompts}
# -------------------------------------------------------------------------
# CRITERION A: Geographic Scope of Application
# -------------------------------------------------------------------------
prompt_scalability_A <- function(txt, title = "", geography = "") {
  context <- ""
  if (!is.na(title) && nzchar(title)) context <- paste0(context, "Document title: ", title, "\n")
  if (!is.na(geography) && nzchar(geography)) context <- paste0(context, "Stated geography: ", geography, "\n")
  
  glue('
Read the following SUMMARY of a grey literature document on Nature-based Solutions spatial prioritisation.

{context}

TASK: Determine the geographic scope at which the spatial analysis was APPLIED (not just discussed or reviewed).

SCORING RULES:
  1 = Local or sub-national: single site, farm, small watershed, municipality, province, district, single river basin, or sub-national region
  2 = National: analysis covers one entire country, or a very large sub-national area approaching country scale
  3 = Multi-country, continental, or global: analysis covers multiple countries, a full continent, worldwide, or provides a globally applicable methodology/tool

IMPORTANT:
- Score based on where the METHOD WAS APPLIED, not where the authors say it could be applied
- If the document applies the method at multiple scales, score the LARGEST scale demonstrated
- A global methodology that only shows local case studies scores based on the demonstrated application
- Methodological frameworks intended for global use (like ROAM, WePlan) can score 3 if they have multi-country applications

RESPONSE FORMAT:
First, write your reasoning in 1-2 sentences naming the specific geography and explaining your score choice. Then write a semicolon (;) followed by ONLY the score digit (1, 2, or 3).

Example: The study applied a weighted overlay analysis across three districts in northern Ethiopia, which is a sub-national extent; 1

SUMMARY:
"""
{txt}
"""')
}

# -------------------------------------------------------------------------
# CRITERION B: Method Transferability
# -------------------------------------------------------------------------
prompt_scalability_B <- function(txt, spatial_approach = "") {
  context <- ""
  if (!is.na(spatial_approach) && nzchar(spatial_approach)) {
    context <- paste0("Stated spatial approach: ", spatial_approach, "\n\n")
  }
  
  glue('
Read the following SUMMARY of a grey literature document on Nature-based Solutions spatial prioritisation.

{context}

TASK: Assess how TRANSFERABLE the spatial method is to other regions without repeating the full study.

SCORING RULES:
  1 = NOT transferable without major effort: method depends on locally calibrated models, local expert or stakeholder workshops for weights, participatory mapping, community-specific training data, or site-specific field calibration that cannot be reused elsewhere. Examples: local AHP with community-derived weights, ML trained exclusively on local field plots, purely participatory GIS exercises.
  2 = MODERATELY transferable: the analytical framework is standard and documented but requires local adaptation such as re-assigning weights, regional retraining of a model, or local parameter tuning. The method structure could be replicated but key parameters are context-specific. Examples: MCDA with transferable criteria but expert-assigned weights; documented toolkit requiring local stakeholder input; process-based model needing regional calibration.
  3 = HIGHLY transferable: method uses a generalizable, rule-based, or globally parameterized approach that can be applied anywhere with minimal adaptation. Examples: published tool with default parameters (ROOT, WePlan, se.plan); simple GIS overlay using only global datasets; fixed threshold-based classification; open-source platform with documented methodology.

IMPORTANT:
- Focus on the METHOD itself, not the data 
- A method that uses global data but requires local expert weighting still scores 2, not 3
- A published tool/platform with fixed parameters and global datasets scores 3
- Consider whether someone in a different country could apply this method from the document alone

RESPONSE FORMAT:
First, write your reasoning in 1-2 sentences explaining what makes the method transferable or not. Then write a semicolon (;) followed by ONLY the score digit (1, 2, or 3).

Example: The ROAM framework provides standardized steps but requires local stakeholder workshops to derive criteria weights for each country application; 2

SUMMARY:
"""
{txt}
"""')
}

# -------------------------------------------------------------------------
# CRITERION C: Data Input Generalizability
# -------------------------------------------------------------------------
prompt_scalability_C <- function(txt) {
  glue('
Read the following SUMMARY of a grey literature document on Nature-based Solutions spatial prioritisation.

TASK: Assess how GENERALIZABLE the input datasets are — could someone replicate this analysis in a different region using the same or equivalent data?

SCORING RULES:
  1 = PRIMARILY LOCAL/PROPRIETARY: the analysis depends mainly on datasets that are not openly available or reproducible elsewhere. Examples: field survey data, local soil sampling campaigns, non-public government inventories, project-specific drone/UAV imagery, local farm census, proprietary commercial data, locally digitized maps, community-collected data.
  2 = MIXED: the analysis uses a combination of globally available open-access datasets AND locally sourced data that would need to be collected or substituted in a new region. Examples: global climate + DEM layers combined with local land use surveys, national statistics not available globally, or local validation datasets.
  3 = PRIMARILY GLOBAL/OPEN-ACCESS: the analysis relies mainly on datasets that are globally available and openly accessible. Examples: WorldClim, CHIRPS, ERA5, CRU, SRTM, ALOS, Copernicus DEM, ESA WorldCover, GlobCover, MODIS products, Sentinel imagery, SoilGrids, iSDA, ISRIC, JRC datasets, GHS population, OpenStreetMap, GADM boundaries, WDPA protected areas. Minor use of local validation data still qualifies as 3 if bulk of inputs are global.

IMPORTANT:
- Look for mentions of specific data sources in the summary
- National datasets that are open and well-documented count as accessible but still national-scope — score 2 unless they have global equivalents
- If the document describes a global tool/platform, it likely uses global datasets (score 3)
- If data sources are not described, infer from the methodology type

RESPONSE FORMAT:
First, write your reasoning in 1-2 sentences listing any data sources mentioned and explaining your score. Then write a semicolon (;) followed by ONLY the score digit (1, 2, or 3).

Example: The WePlan platform uses Copernicus land cover, IPCC carbon defaults, and global species distribution models as its main inputs, all globally available; 3

SUMMARY:
"""
{txt}
"""')
}
```

# ============================================================================
# PART 4: RUN SCALABILITY SCORING
# ============================================================================

```{r run_scalability_scoring, eval=FALSE}
n <- nrow(docs_to_score)

# Pre-allocate vectors
score_A <- integer(n)
score_B <- integer(n)
score_C <- integer(n)
justification_A <- character(n)
justification_B <- character(n)
justification_C <- character(n)

pb <- txtProgressBar(min = 0, max = n, style = 3)

for (i in seq_len(n)) {
  # Get text and metadata
  txt <- docs_to_score$GPT_Summary[i]
  txt_clipped <- clip_text(txt, MAX_CHARS)
  
  # Get additional context if available
  title <- docs_to_score$Source_title[i] %||% ""
  geography <- docs_to_score$Geography[i] %||% ""
  spatial_approach <- docs_to_score$Spatial_approach[i] %||% ""

  # --- A: Geographic Scope ---
  resp_A <- safe_chat(prompt_scalability_A(txt_clipped, title, geography))
  parsed_A <- parse_score_response(resp_A$response)
  score_A[i] <- parsed_A$score
  justification_A[i] <- parsed_A$justification
  Sys.sleep(0.5)

  # --- B: Method Transferability ---
  resp_B <- safe_chat(prompt_scalability_B(txt_clipped, spatial_approach))
  parsed_B <- parse_score_response(resp_B$response)
  score_B[i] <- parsed_B$score
  justification_B[i] <- parsed_B$justification
  Sys.sleep(0.5)

  # --- C: Data Input Generalizability ---
  resp_C <- safe_chat(prompt_scalability_C(txt_clipped))
  parsed_C <- parse_score_response(resp_C$response)
  score_C[i] <- parsed_C$score
  justification_C[i] <- parsed_C$justification

  setTxtProgressBar(pb, i)
  Sys.sleep(0.5)
}

close(pb)

# Attach scores to dataframe
docs_to_score$Scalability_A_Geographic    <- score_A
docs_to_score$Scalability_A_Justification <- justification_A
docs_to_score$Scalability_B_Method        <- score_B
docs_to_score$Scalability_B_Justification <- justification_B
docs_to_score$Scalability_C_Data          <- score_C
docs_to_score$Scalability_C_Justification <- justification_C

# Scalability Score = median(A, B, C)
docs_to_score$Scalability_Score <- apply(
  docs_to_score[, c("Scalability_A_Geographic",
                     "Scalability_B_Method",
                     "Scalability_C_Data")],
  1,
  function(row) {
    vals <- na.omit(row)
    if (length(vals) == 0) return(NA_integer_)
    as.integer(round(median(vals)))
  }
)

message("\n✓ Scalability scoring complete!")

# Save intermediate results
saveRDS(docs_to_score, file.path(OUT_DIR, "grey_lit_scalability_scored.rds"))
```

# ============================================================================
# PART 5: EVIDENCE QUALITY SCORING (Grey Literature Adaptation)
# ============================================================================

```{r quality_scoring}
# ============================================================================
# GREY LITERATURE EVIDENCE QUALITY SCORING
# ============================================================================
#
# Unlike peer-reviewed papers, grey literature lacks journal impact factors.
# We use a three-component scoring system:
#
# COMPONENT 1: Organizational Authority (from Organisation column)
#   1 = NGO, consultant, university project (no formal review)
#   2 = National government, bilateral agency, major research institute
#   3 = International institution (UN agencies, IPCC, JRC, World Bank, FAO, IUCN global)
#
# COMPONENT 2: Document Rigor (from Document type + content)
#   1 = Guidelines/toolkit without validation, policy brief, poster
#   2 = Technical report with case studies, working paper with analysis
#   3 = Methodological paper with validation, peer-reviewed grey lit, multi-country assessment
#
# COMPONENT 3: Citation Impact (from citations column)
#   1 = <20 citations
#   2 = 20-100 citations
#   3 = >100 citations
#
# Quality_Score = mode or median of three components
# ============================================================================

# Load scored data if running separately
if (!exists("docs_to_score") || !"Scalability_Score" %in% names(docs_to_score)) {
  if (file.exists(file.path(OUT_DIR, "grey_lit_scalability_scored.rds"))) {
    docs_to_score <- readRDS(file.path(OUT_DIR, "grey_lit_scalability_scored.rds"))
  }
}

# ----- Component 1: Organizational Authority -----
score_org_authority <- function(org) {
  if (is.na(org) || !nzchar(org)) return(1L)
  org_lower <- tolower(org)
  
  # Score 3: International institutions
  intl_patterns <- c(
    "iucn", "fao", "unep", "undp", "un-redd", "unfccc", "ipcc", "ipbes",
    "world bank", "gef", "jrc", "european commission", "oecd", 
    "cbd", "ramsar", "cites", "unccd", "wri", "wmo", "who",
    "cifor", "icraf", "ciat", "cgiar", "alliance bioversity",
    "natural capital project", "itto"
  )
  if (any(sapply(intl_patterns, function(p) grepl(p, org_lower)))) return(3L)
  
  # Score 2: National government, bilateral agencies, major research
  natl_patterns <- c(
    "ministry", "government", "department of", "national", "federal",
    "usda", "usaid", "giz", "dfid", "jica", "sida", "norad",
    "forest service", "epa", "environment agency",
    "university", "research", "institute", "foundation"
  )
  if (any(sapply(natl_patterns, function(p) grepl(p, org_lower)))) return(2L)
  
  # Score 1: Default (NGO, consultant, other)
  return(1L)
}

# ----- Component 2: Document Rigor -----
score_doc_rigor <- function(doc_type, confidence = NA) {
  if (is.na(doc_type) || !nzchar(doc_type)) {
    # Fall back to confidence if available
    if (!is.na(confidence)) {
      conf_lower <- tolower(confidence)
      if (grepl("high", conf_lower)) return(3L)
      if (grepl("medium|med", conf_lower)) return(2L)
      return(1L)
    }
    return(1L)
  }
  
  doc_lower <- tolower(doc_type)
  
  # Score 3: High rigor
  high_rigor <- c(
    "methodology", "methodological", "validation", "peer",
    "occasional paper", "assessment report", "multi-country"
  )
  if (any(sapply(high_rigor, function(p) grepl(p, doc_lower)))) return(3L)
  
  # Score 2: Medium rigor
  med_rigor <- c(
    "technical report", "working paper", "analytical", "case stud",
    "technical note", "tool documentation", "general technical"
  )
  if (any(sapply(med_rigor, function(p) grepl(p, doc_lower)))) return(2L)
  
  # Score 1: Lower rigor
  low_rigor <- c(
    "guideline", "toolkit", "handbook", "manual", "brief",
    "poster", "factsheet", "brochure"
  )
  if (any(sapply(low_rigor, function(p) grepl(p, doc_lower)))) return(1L)
  
  # Check confidence as fallback
  if (!is.na(confidence)) {
    conf_lower <- tolower(confidence)
    if (grepl("high", conf_lower)) return(3L)
    if (grepl("medium|med", conf_lower)) return(2L)
    return(1L)
  }
  
  return(2L)  # Default to medium if unclear
}

# ----- Component 3: Citation Impact -----
score_citations <- function(cites) {
  if (is.na(cites)) return(1L)
  cites <- as.numeric(cites)
  if (is.na(cites)) return(1L)
  
  if (cites > 100) return(3L)
  if (cites >= 20) return(2L)
  return(1L)
}

# ----- Apply Quality Scoring -----
docs_to_score <- docs_to_score %>%
  mutate(
    # Individual components
    Quality_Org = map_int(
      Organisation_authoring_body %||% Organisation, 
      score_org_authority
    ),
    Quality_Rigor = map2_int(
      Document_type, 
      Confidence,
      score_doc_rigor
    ),
    Quality_Citations = map_int(citations, score_citations),
    
    # Combined Quality Score (median of three components)
    Quality_Score = pmap_int(
      list(Quality_Org, Quality_Rigor, Quality_Citations),
      function(a, b, c) {
        vals <- na.omit(c(a, b, c))
        if (length(vals) == 0) return(1L)
        as.integer(round(median(vals)))
      }
    )
  )

# ----- Quality diagnostics -----
cat("\n=== EVIDENCE QUALITY COMPONENT DISTRIBUTIONS ===\n")
cat("\nOrganizational Authority:\n")
print(table(docs_to_score$Quality_Org, useNA = "ifany"))
cat("\nDocument Rigor:\n")
print(table(docs_to_score$Quality_Rigor, useNA = "ifany"))
cat("\nCitation Impact:\n")
print(table(docs_to_score$Quality_Citations, useNA = "ifany"))
cat("\nCombined Quality Score:\n")
print(table(docs_to_score$Quality_Score, useNA = "ifany"))
```

# ============================================================================
# PART 6: COMPOSITE BENCHMARK
# ============================================================================

```{r composite_benchmark}
# ============================================================================
# COMPOSITE BENCHMARK FLAG
# ============================================================================
#
# Using the same matrix logic as peer-reviewed:
#
#                  Quality=1   Quality=2   Quality=3
# Scalability=1     Low         Low         Low
# Scalability=2     Low         Medium      High
# Scalability=3     Low         High        High
#
# High:   Scalability >= 2 AND Quality >= 2, but NOT (Scalability=2 AND Quality=2)
# Medium: Scalability = 2 AND Quality = 2
# Low:    Scalability = 1 OR Quality = 1
# ============================================================================

docs_to_score <- docs_to_score %>%
  mutate(
    Final_Benchmark = case_when(
      is.na(Scalability_Score) | is.na(Quality_Score)         ~ "Unscored",
      # Low: either dimension is 1
      Scalability_Score == 1 | Quality_Score == 1             ~ "Low",
      # Medium: both are exactly 2
      Scalability_Score == 2 & Quality_Score == 2             ~ "Medium",
      # High: Scalability >= 2 AND Quality >= 2 (and not both exactly 2)
      Scalability_Score >= 2 & Quality_Score >= 2             ~ "High",
      # Fallback
      TRUE                                                    ~ "Low"
    ),
    
    # Numeric combined score for ranking (average of both dimensions)
    Combined_Score = case_when(
      is.na(Scalability_Score) | is.na(Quality_Score) ~ NA_real_,
      TRUE ~ (as.numeric(Scalability_Score) + as.numeric(Quality_Score)) / 2
    )
  )

cat("\n=== FINAL BENCHMARK DISTRIBUTION ===\n")
print(table(docs_to_score$Final_Benchmark, useNA = "ifany"))
```

# ============================================================================
# PART 7: SUMMARIES & DIAGNOSTICS
# ============================================================================

```{r summaries}
# --- Overall distributions ---
cat("\n=== SCALABILITY SUB-SCORE DISTRIBUTIONS ===\n")
cat("\nA. Geographic Scope:\n")
print(table(docs_to_score$Scalability_A_Geographic, useNA = "ifany"))
cat("\nB. Method Transferability:\n")
print(table(docs_to_score$Scalability_B_Method, useNA = "ifany"))
cat("\nC. Data Input Generalizability:\n")
print(table(docs_to_score$Scalability_C_Data, useNA = "ifany"))
cat("\nScalability Score (median):\n")
print(table(docs_to_score$Scalability_Score, useNA = "ifany"))

# --- Cross-tabulation: Scalability × Quality ---
cat("\n=== SCALABILITY × QUALITY CROSS-TABLE ===\n")
print(table(
  Scalability = docs_to_score$Scalability_Score,
  Quality = docs_to_score$Quality_Score,
  useNA = "ifany"
))

# --- By NbS Group ---
cat("\n=== BENCHMARK BY NbS GROUP ===\n")
if ("NbS_Group" %in% names(docs_to_score)) {
  docs_to_score %>%
    group_by(NbS_Group) %>%
    summarise(
      n = n(),
      mean_scalability = round(mean(Scalability_Score, na.rm = TRUE), 1),
      mean_quality     = round(mean(Quality_Score, na.rm = TRUE), 1),
      mean_combined    = round(mean(Combined_Score, na.rm = TRUE), 1),
      n_high   = sum(Final_Benchmark == "High", na.rm = TRUE),
      n_medium = sum(Final_Benchmark == "Medium", na.rm = TRUE),
      n_low    = sum(Final_Benchmark == "Low", na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(desc(mean_combined)) %>%
    print(n = 30)
}

# --- By Spatial Approach ---
cat("\n=== BENCHMARK BY SPATIAL APPROACH ===\n")
if ("Spatial_approach" %in% names(docs_to_score)) {
  docs_to_score %>%
    group_by(Spatial_approach) %>%
    summarise(
      n = n(),
      mean_combined = round(mean(Combined_Score, na.rm = TRUE), 1),
      pct_high      = round(100 * mean(Final_Benchmark == "High", na.rm = TRUE), 0),
      pct_medium    = round(100 * mean(Final_Benchmark == "Medium", na.rm = TRUE), 0),
      pct_low       = round(100 * mean(Final_Benchmark == "Low", na.rm = TRUE), 0),
      .groups = "drop"
    ) %>%
    arrange(desc(mean_combined)) %>%
    print()
}

# --- By Document Type ---
cat("\n=== BENCHMARK BY DOCUMENT TYPE ===\n")
if ("Document_type" %in% names(docs_to_score)) {
  docs_to_score %>%
    group_by(Document_type) %>%
    summarise(
      n = n(),
      mean_combined = round(mean(Combined_Score, na.rm = TRUE), 1),
      pct_high = round(100 * mean(Final_Benchmark == "High", na.rm = TRUE), 0),
      .groups = "drop"
    ) %>%
    arrange(desc(mean_combined)) %>%
    print(n = 20)
}

# --- Top documents by combined score ---
cat("\n=== TOP 20 DOCUMENTS BY COMBINED SCORE ===\n")
docs_to_score %>%
  filter(!is.na(Combined_Score)) %>%
  arrange(desc(Combined_Score), desc(citations)) %>%
  select(
    any_of(c("Source_title", "NbS_Group", "Organisation_authoring_body",
             "Scalability_Score", "Quality_Score", "Combined_Score", 
             "Final_Benchmark", "citations"))
  ) %>%
  head(20) %>%
  print()

# --- Documents needing attention (Low benchmark) ---
cat("\n=== LOW BENCHMARK DOCUMENTS ===\n")
docs_to_score %>%
  filter(Final_Benchmark == "Low") %>%
  select(
    any_of(c("Source_title", "NbS_Group", "Scalability_Score", 
             "Quality_Score", "Final_Benchmark"))
  ) %>%
  print(n = 50)
```

# ============================================================================
# PART 8: COMPARISON WITH EXISTING CONFIDENCE RATINGS
# ============================================================================

```{r confidence_comparison}
# Compare GPT-derived Quality with original Confidence column
if ("Confidence" %in% names(docs_to_score)) {
  cat("\n=== COMPARISON: Quality Score vs Original Confidence ===\n")
  
  docs_to_score <- docs_to_score %>%
    mutate(
      Confidence_Numeric = case_when(
        tolower(Confidence) == "low" ~ 1L,
        tolower(Confidence) %in% c("low-med", "low-medium") ~ 1L,
        tolower(Confidence) == "medium" ~ 2L,
        tolower(Confidence) %in% c("med", "med-high") ~ 2L,
        tolower(Confidence) == "high" ~ 3L,
        TRUE ~ NA_integer_
      )
    )
  
  # Cross-tabulation
  cat("\nQuality Score vs Original Confidence:\n")
  print(table(
    Quality = docs_to_score$Quality_Score,
    Confidence = docs_to_score$Confidence,
    useNA = "ifany"
  ))
  
  # Agreement rate
  agreement <- docs_to_score %>%
    filter(!is.na(Quality_Score), !is.na(Confidence_Numeric)) %>%
    summarise(
      n = n(),
      exact_match = sum(Quality_Score == Confidence_Numeric),
      within_one = sum(abs(Quality_Score - Confidence_Numeric) <= 1),
      agreement_pct = round(100 * exact_match / n, 1),
      close_agreement_pct = round(100 * within_one / n, 1)
    )
  
  cat(glue("\nAgreement statistics:
  Total compared: {agreement$n}
  Exact match: {agreement$exact_match} ({agreement$agreement_pct}%)
  Within 1 point: {agreement$within_one} ({agreement$close_agreement_pct}%)\n"))
}
```

# ============================================================================
# PART 9: SAVE RESULTS
# ============================================================================

```{r save_final}
# Prepare final output
grey_lit_final <- docs_to_score %>%
  select(
    # Original identifiers
    any_of(c("NbS_Group", "ID", "Source_title", "Organisation_authoring_body",
             "Year", "Document_type", "Geography", "Spatial_approach")),
    # Scalability scores
    Scalability_A_Geographic, Scalability_A_Justification,
    Scalability_B_Method, Scalability_B_Justification,
    Scalability_C_Data, Scalability_C_Justification,
    Scalability_Score,
    # Quality scores
    Quality_Org, Quality_Rigor, Quality_Citations, Quality_Score,
    # Combined benchmark
    Combined_Score, Final_Benchmark,
    # Original metadata
    any_of(c("citations", "Confidence", "Link", "Why_it_matters_1_sentence"))
  )

# Save in multiple formats
saveRDS(grey_lit_final, file.path(OUT_DIR, "grey_lit_benchmarked.rds"))
write_csv(grey_lit_final, file.path(OUT_DIR, "grey_lit_benchmarked.csv"))
write_xlsx(grey_lit_final, file.path(OUT_DIR, "grey_lit_benchmarked.xlsx"))

# Save full dataset with all columns
saveRDS(docs_to_score, file.path(OUT_DIR, "grey_lit_benchmarked_full.rds"))

message(glue("\n✓ Benchmarked results saved to: {OUT_DIR}"))
message(glue("  - grey_lit_benchmarked.rds"))
message(glue("  - grey_lit_benchmarked.csv"))
message(glue("  - grey_lit_benchmarked.xlsx"))
message(glue("  - grey_lit_benchmarked_full.rds (includes GPT summaries)"))

cat("\n=== FINAL COLUMNS ===\n")
cat(paste(names(grey_lit_final), collapse = "\n"))
```

# ============================================================================
# PART 10: INTERACTIVE PREVIEW
# ============================================================================

```{r preview_final}
DT::datatable(
  grey_lit_final %>%
    mutate(across(where(is.character), ~ifelse(
      nchar(.) > 80, paste0(substr(., 1, 80), "..."), .
    ))),
  options = list(pageLength = 15, scrollX = TRUE),
  caption = "Grey Literature Benchmarking — Scalability + Evidence Quality"
)
```

# ============================================================================
# PART 11: COST ESTIMATE
# ============================================================================

```{r cost}
if (exists("docs_to_score")) {
  n_docs <- nrow(docs_to_score)
  total_calls <- n_docs * 3  # 3 scalability prompts per document
  
  # Estimate based on GPT Summary lengths
  if ("GPT_Summary" %in% names(docs_to_score)) {
    avg_chars <- mean(nchar(docs_to_score$GPT_Summary), na.rm = TRUE)
  } else {
    avg_chars <- 5000  # Default estimate
  }
  
  avg_input_tokens <- avg_chars / 4
  avg_output_tokens <- 80  # Short responses with justification

  # GPT-4o-mini pricing
  input_cost <- (avg_input_tokens * total_calls / 1e6) * 0.15
  output_cost <- (avg_output_tokens * total_calls / 1e6) * 0.60
  total_cost <- input_cost + output_cost

  cat(glue("\n=== BENCHMARKING COST ESTIMATE ===\n"))
  cat(glue("Documents: {n_docs}\n"))
  cat(glue("API calls: {total_calls} (3 per document)\n"))
  cat(glue("Avg input tokens per call: ~{round(avg_input_tokens)}\n"))
  cat(glue("Estimated cost (GPT-4o-mini): ${round(total_cost, 2)}\n"))
  cat("=================================\n")
}
```

# ============================================================================
# OPTIONAL: TEST ON SMALL SAMPLE
# ============================================================================

```{r test_sample, eval=FALSE}
# Run this chunk manually to test on a small sample before full run
set.seed(42)

# Sample 2 docs per NbS Group
test_sample <- docs_to_score %>%
  group_by(NbS_Group) %>%
  slice_head(n = 2) %>%
  ungroup() %>%
  head(10)  # Max 10 for testing

message(glue("Testing benchmarking on {nrow(test_sample)} documents..."))

n_test <- nrow(test_sample)
test_A <- integer(n_test); test_B <- integer(n_test); test_C <- integer(n_test)
test_just_A <- character(n_test); test_just_B <- character(n_test); test_just_C <- character(n_test)

for (i in seq_len(n_test)) {
  txt <- test_sample$GPT_Summary[i]
  txt_clipped <- clip_text(txt, MAX_CHARS)
  
  title <- test_sample$Source_title[i] %||% ""
  geography <- test_sample$Geography[i] %||% ""
  spatial_approach <- test_sample$Spatial_approach[i] %||% ""

  resp_A <- safe_chat(prompt_scalability_A(txt_clipped, title, geography))
  parsed_A <- parse_score_response(resp_A$response)
  test_A[i] <- parsed_A$score; test_just_A[i] <- parsed_A$justification

  resp_B <- safe_chat(prompt_scalability_B(txt_clipped, spatial_approach))
  parsed_B <- parse_score_response(resp_B$response)
  test_B[i] <- parsed_B$score; test_just_B[i] <- parsed_B$justification

  resp_C <- safe_chat(prompt_scalability_C(txt_clipped))
  parsed_C <- parse_score_response(resp_C$response)
  test_C[i] <- parsed_C$score; test_just_C[i] <- parsed_C$justification

  cat(glue("\n[{i}/{n_test}] {substr(test_sample$Source_title[i], 1, 50)}: A={test_A[i]} B={test_B[i]} C={test_C[i]}"))
  Sys.sleep(1)
}

test_sample$Scalability_A <- test_A; test_sample$Scalability_A_Just <- test_just_A
test_sample$Scalability_B <- test_B; test_sample$Scalability_B_Just <- test_just_B
test_sample$Scalability_C <- test_C; test_sample$Scalability_C_Just <- test_just_C
test_sample$Scalability_Score <- apply(
  test_sample[, c("Scalability_A", "Scalability_B", "Scalability_C")],
  1, function(r) as.integer(round(median(na.omit(r))))
)

# Apply quality scoring
test_sample <- test_sample %>%
  mutate(
    Quality_Org = map_int(Organisation_authoring_body, score_org_authority),
    Quality_Rigor = map2_int(Document_type, Confidence, score_doc_rigor),
    Quality_Citations = map_int(citations, score_citations),
    Quality_Score = pmap_int(
      list(Quality_Org, Quality_Rigor, Quality_Citations),
      function(a, b, c) as.integer(round(median(na.omit(c(a, b, c)))))
    )
  )

# Preview test results
test_sample %>%
  select(Source_title, NbS_Group,
         Scalability_A, Scalability_B, Scalability_C, Scalability_Score,
         Quality_Org, Quality_Rigor, Quality_Citations, Quality_Score) %>%
  print(n = 15)

# Save test results
write_csv(
  test_sample %>% select(-GPT_Summary),
  file.path(OUT_DIR, "grey_lit_test_results.csv")
)
```

# ============================================================================
# METHODOLOGY NOTES
# ============================================================================

## Key Adaptations from Peer-Reviewed Benchmarking

### Dimension 1: Scalability (same logic, different input)
- **Input**: GPT Summary text instead of full PDF
- **Prompts**: Adapted to work with summary-level detail
- **Scoring**: Identical 1-3 scale for Geographic Scope, Method Transferability, Data Generalizability

### Dimension 2: Evidence Quality (grey-lit specific)
Replaces bibliometric scoring (Journal IF + citations) with:

| Component | Score 1 | Score 2 | Score 3 |
|-----------|---------|---------|---------|
| **Organizational Authority** | NGO, consultant | National govt, bilateral agency | International institution (UN, CGIAR, JRC) |
| **Document Rigor** | Guidelines, toolkit | Technical report with cases | Methodological paper, validated tool |
| **Citation Impact** | <20 citations | 20-100 citations | >100 citations |

### Composite Benchmark Matrix (unchanged)
|                  | Quality=1 | Quality=2 | Quality=3 |
|------------------|-----------|-----------|-----------|
| **Scalability=1** | Low      | Low       | Low       |
| **Scalability=2** | Low      | Medium    | High      |
| **Scalability=3** | Low      | High      | High      |
